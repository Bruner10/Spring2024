---
title: "Homework 6"
author: "Jonathan Bruner"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tree)
library(randomForest)
library(plotmo)
library(gbm)
library(BART)
library(ISLR2)
library(glmnet)
```

# Question 1

## Apply boosting, bagging, random forests and BART to the *gene.csv* dataset to predict ***out*** using the expression levels of the 50 genes. Fit the models on a training set containing the first 50 observations and evaluate their performance on a test set containing the last 10 observations. Which of these approaches yields the best performance?

```{r message=FALSE, warning=FALSE}
set.seed(10)

dat1 = read.csv('gene.csv')
dat1.train = dat1[1:50,]
dat1.test = dat1[-1:-50,]

bag1 = randomForest(out ~ ., data=dat1.train, mtry=(ncol(dat1)-1), importance=TRUE)
boost1 = gbm(out ~ ., data=dat1.train, distribution='gaussian', interaction.depth=4)
rf1 = randomForest(out ~ ., data=dat1.train, mtry=3, importance=TRUE)
bart1 = gbart(dat1.train[,-1], dat1.train[,1], x.test = dat1.test[,-1])

bag1.pred = predict(bag1, dat1.test)
boost1.pred = predict(boost1, dat1.test)
rf1.pred = predict(rf1, dat1.test)
bart1.pred = bart1$yhat.test.mean
```

Boosting creates the best results.

```{r}
data.frame(Models=c('Bagging','Boosting','Random Forest','BART'),
           MSE=c(mean((dat1.test$out - bag1.pred)^2), 
                 mean((dat1.test$out - boost1.pred)^2), 
                 mean((dat1.test$out - rf1.pred)^2), 
                 mean((dat1.test$out - bart1.pred)^2)
                 )
           )
```

<br><br><br>

***

# Question 2: Chapter 8 Exercise 9 excluding (c)

## This problem involves the *OJ* data set which is part of the ***ISLR2*** package.

### (a)   Create a training set containing a random sample of 800 observations, and a test set containing the remaining observations.

```{r}
set.seed(42)

dat2 = OJ

train = sample(1:nrow(dat2), 800)

dat2.train = dat2[train,]
dat2.test = dat2[-train,]
```

<br>

### (b)   Fit a tree to the training data, with ***Purchase*** as the response and the other variables as predictors. Use the **summary()** function to produce summary statistics about the tree, and describe the results obtained. What is the training error rate? How many terminal nodes does the tree have?

```{r}
tree2 = tree(Purchase ~ ., data=dat2, subset=train)
summary(tree2)
```

The training error rate is 0.1472

<br>

### (d)   Create a plot of the tree, and interpret the results.

```{r fig.height=5}
plot(tree2)
text(tree2, cex=0.5)
```

This tree has 8 terminal nodes, using 4 of the variables provided as predictors (***LoyalCH***, ***PriceDiff***, ***StoreID***, and ***ListPriceDiff***).

<br>

### (e)   Predict the response on the test data, and produce a confusion matrix comparing the test labels to the predicted test labels. What is the test error rate?

```{r}
tree2.pred = predict(tree2, dat2.test)
tree2.pred.val = ifelse(tree2.pred[,1] > 0.5,'CH','MM')

table(dat2.test$Purchase, tree2.pred.val)
mean(dat2.test$Purchase != tree2.pred.val)
```

<br>

### (f)   Apply the **cv.tree()** function to the training set in order to determine the optimal tree size.

```{r}
cvtree2 = cv.tree(tree2)
```

<br>

### (g)   Produce a plot with tree size on the x-axis and cross-validated classification error rate on the y-axis.

```{r}
plot(cvtree2$size, cvtree2$dev, type='b', xlab='Size', ylab='Dev')
```

<br>

### (h)   Which tree size corresponds to the lowest cross-validated classification error rate?

We want a tree of size `r cvtree2$size[which.min(cvtree2$dev)]` to minimize classification error rate.

<br>

### (i)   Produce a pruned tree corresponding to the optimal tree size obtained using cross-validation. If cross-validation does not lead to selection of a pruned tree, then create a pruned tree with five terminal nodes.

```{r}
prunedtree2 = prune.tree(tree2, best = cvtree2$size[which.min(cvtree2$dev)])
plot(prunedtree2)
text(prunedtree2, cex=0.5)
```

<br>

### (j)   Compare the training error rates between the pruned and unpruned trees. Which is higher?

```{r}
tree2.train.pred = predict(tree2, dat2.train)
prunedtree2.train.pred = predict(prunedtree2, dat2.train)

tree2.train.pred.val = ifelse(tree2.train.pred[,1] > 0.5,'CH','MM')
prunedtree2.train.pred.val = ifelse(prunedtree2.train.pred[,1] > 0.5,'CH','MM')

data.frame(
  Trees = c('Unpruned Tree','Pruned Tree'),
  Error = c(mean(dat2.train$Purchase != tree2.train.pred.val),
            mean(dat2.train$Purchase != prunedtree2.train.pred.val))
)
```

The Pruned Tree's error rate is higher.

<br>

### (k)   Compare the test error rates between the pruned and unpruned trees. Which is higher?

```{r}
tree2.test.pred = predict(tree2, dat2.test)
prunedtree2.test.pred = predict(prunedtree2, dat2.test)

tree2.test.pred.val = ifelse(tree2.test.pred[,1] > 0.5,'CH','MM')
prunedtree2.test.pred.val = ifelse(prunedtree2.test.pred[,1] > 0.5,'CH','MM')

data.frame(
  Trees = c('Unpruned Tree','Pruned Tree'),
  Error = c(mean(dat2.test$Purchase != tree2.test.pred.val),
            mean(dat2.test$Purchase != prunedtree2.test.pred.val))
)
```

The Pruned Tree's error rate is higher.

<br><br><br>

***

# Question 3: Chapter 8 Exercise 10

## We now use boosting to predict ***Salary*** in the **Hitters** data set.

### (a)   Remove the observations for whome the salary information is unknown, and then log-transform the salaries.

```{r}
dat3 = na.omit(Hitters)
dat3$Salary = log(dat3$Salary)
```

<br>

### (b)   Create a training set consisting of the first 200 observations, and a test set consisting of the remaining observations.

```{r}
set.seed(21)

dat3.train = dat3[1:200,]
dat3.test = dat3[-1:-200,]
```

<br>

### (c)   Perform boosting on the training set with 1,000 trees for a range of values of the shrinkage parameter $\lambda$. Produce a plot with different shrinkage values on the x-axis and the corresponding training set MSE on the y-axis.

```{r}
range = c(0.01, 0.05, 0.1, 0.15, 0.2)
train.MSE = NA
test.MSE = NA

for (i in range){
  boost = gbm(Salary ~ ., data=dat3.train, distribution='gaussian', n.trees=1000, shrinkage=i)
  pred = predict(boost, dat3.train, n.trees=1000)
  train.MSE[which(range==i)] = mean((dat3.train$Salary - pred)^2)
  pred = predict(boost, dat3.test, n.trees=1000)
  test.MSE[which(range==i)] = mean((dat3.test$Salary - pred)^2)
}

plot(range, train.MSE, type='b', xlab='Shrinkage', ylab='MSE')

```

<br>

### (d)   Produce a plot with different shrinkage values on the x-axis and the corresponding test set MSE on the y-axis.

```{r}
plot(range, test.MSE, type='b', xlab='Shrinkage', ylab='MSE')
```

<br>

### (e)   Compare the test MSE of boosting to the test MSE that results from applying two of the regression approaches seen in Chapters 3 and 6.

```{r}
xtrain = model.matrix(Salary ~ ., dat3.train)[,-1]
xtest = model.matrix(Salary ~ ., dat3.test)[,-1]
ytrain = dat3.train[,which(names(dat3.train)=='Salary')]
ytest = dat3.test[,which(names(dat3.test)=='Salary')]

linear3 = lm(Salary ~ ., data=dat3.train)
linear3.pred = predict(linear3, dat3.test)
linear3.mse = mean((linear3.pred - dat3.test$Salary)^2)

lasso3 = cv.glmnet(xtrain, ytrain, alpha=1)
lasso3.min.lambda = lasso3$lambda.min
lasso3.pred = predict(lasso3, s = lasso3.min.lambda, newx = xtest)
lasso3.mse = mean((lasso3.pred - ytest)^2)

boost3 = gbm(Salary ~ ., data=dat3.train, distribution='gaussian', n.trees=1000, shrinkage=0.1)
boost3.pred = predict(boost3, dat3.test, n.trees=1000)
boost3.mse = mean((dat3.test$Salary - boost3.pred)^2)

data.frame(
  Models = c('Boosted Tree','Linear Regression','Lasso'),
  MSE = c(boost3.mse,linear3.mse,lasso3.mse)
)
```

<br>

### (f)   Which variables appear to be the most important predictors in the boosted model?

It appears that ***CHmRun*** is the most influential predictor followed by ***PutOuts*** and ***Walks***

<br>

### (g)   Now apply bagging to the training set. What is the test set MSE for this approach?

```{r}
bag3 = randomForest(Salary ~ ., data=dat3.train, mtry=(ncol(dat3.train)-1), importance=TRUE)
bag3.pred = predict(bag3, dat3.test)
bag3.mse = mean((dat3.test$Salary - bag3.pred)^2)
```

The test set MSE for this approach is `r bag3.mse`.

<br><br><br>

***

# Question 4: Chapter 8 Exercise 1

## Draw and example (of your own invention) of a partition of two-dimensional feature space that could result from recursive binary splitting. Your example should contain at least six regions. Draw a decision tree corresponding to this partition. Be sure to label all aspects of your figures, including the regions $R_1,R_2,\dots$, the cutpoints $t_1,t_2,\dots$, and so forth.

<br>

 ![](C:/Users/JonathanBruner/OneDrive - PakEnergy/Documents/Spring2024/HW6/Question4.png)

<br>

***

# Question 5: Chapter 8 Exercise 5

## Suppose we produce ten bootstrapped samples from a data set containing red and green classes. We then apply a classification tree to each bootstrapped sample and, for a specific value of $X$, produce 10 estimates of $P(\text{Class is Red}|X)$:$$0.1,0.15,0.2,0.2,0.55,0.6,0.6,0.65,0.7, \text{and } 0.75$$There are two common ways to combine these results together into a single class prediction. One is the majority vote approach discussed in this chapter. The second approach is to classify based on the average probability. In this example, what is the final classification under each of these two approaches?

Using the first approach the final classification would be "Red", because there are 4 observations that are "Green" and 6 observations that are "Red". 

Using the second approach the final classification would be "Green", because the mean of all the numbers is 0.45, which means the higher probability is "Green".

<br><br>